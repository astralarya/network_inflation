{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bca5ced7",
   "metadata": {},
   "source": [
    "# Resnet Mutate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507a0491-3708-4d82-ae1b-f1a82b5a6069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.13.1\n",
      "Torchvision Version:  0.14.1a0\n",
      "Pytorch device:  <function device at 0x15b5e1ea0>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from local import data\n",
    "from local import resnet\n",
    "from local import train\n",
    "from local.device import device, cpu\n",
    "\n",
    "\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"Torchvision Version: \", torchvision.__version__)\n",
    "print(\"Pytorch device: \", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7104c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data `data/imagenet-1k/train/`... DONE\n"
     ]
    }
   ],
   "source": [
    "train_data = data.load_dataset(\"data/imagenet-1k/train/\", transform=data.train_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "624ca1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, resnet18 = resnet.network_load(\"resnet18\", reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a9654cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0128,  0.0479, -0.0319, -0.0065,  0.0172, -0.0127,  0.0236],\n",
       "        [-0.0112,  0.0237, -0.0417,  0.0009, -0.0344, -0.0086, -0.0208],\n",
       "        [-0.0221, -0.0088, -0.0249, -0.0334, -0.0476,  0.0426,  0.0486],\n",
       "        [ 0.0141,  0.0225,  0.0007,  0.0146,  0.0534, -0.0150,  0.0193],\n",
       "        [-0.0125,  0.0142, -0.0313,  0.0240, -0.0588, -0.0233, -0.0187],\n",
       "        [-0.0162, -0.0030, -0.0189, -0.0160,  0.0033,  0.0306, -0.0607],\n",
       "        [ 0.0177, -0.0135, -0.0114, -0.0032,  0.0513, -0.0081,  0.0197]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18.get_parameter('conv1.weight')[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05993d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    resnet18.get_parameter('conv1.weight')[0, 0, 0, 0].copy_(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3c2da38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.0000e+00, -3.1703e-02,  2.4431e-02,  2.5522e-02,  2.4349e-03,\n",
       "          5.8660e-02, -7.6692e-02],\n",
       "        [ 3.1217e-02, -7.9921e-02,  5.3224e-02,  2.6282e-02,  5.6118e-02,\n",
       "         -1.5078e-02,  2.1643e-02],\n",
       "        [ 4.0474e-02, -6.4064e-02,  7.6471e-02,  1.0004e-02, -7.1027e-02,\n",
       "         -4.9162e-02,  7.5553e-03],\n",
       "        [ 6.1721e-02, -5.7884e-02,  6.4272e-02,  5.2553e-03,  6.9369e-02,\n",
       "         -4.5745e-02,  2.4190e-02],\n",
       "        [-1.9684e-02,  1.9963e-02,  2.8986e-02,  2.2048e-02,  1.7536e-02,\n",
       "         -5.9144e-02, -5.8423e-02],\n",
       "        [-4.2318e-02,  7.9948e-02, -3.9535e-02,  1.2453e-02, -5.1993e-02,\n",
       "          5.4344e-02, -1.9071e-02],\n",
       "        [-4.1960e-02,  5.0471e-02, -4.5170e-02,  4.7843e-02,  5.5428e-02,\n",
       "          4.1645e-02, -4.9374e-02]], device='mps:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18.get_parameter('conv1.weight')[0, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "network_inflation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "2620dbd39af60e1660afb0b48f7f7a20401ec58f770d6c519b6245c85516b113"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
